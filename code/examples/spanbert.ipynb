{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ark_nlp.model.ner.span_bert import SpanBert\n",
    "from ark_nlp.model.ner.span_bert import SpanBertConfig\n",
    "from ark_nlp.model.ner.span_bert import Dataset\n",
    "from ark_nlp.model.ner.span_bert import Task\n",
    "from ark_nlp.factory.optimizer import get_w2ner_model_optimizer as get_default_model_optimizer\n",
    "from ark_nlp.factory.lr_scheduler import get_default_cosine_schedule_with_warmup\n",
    "from ark_nlp.model.ner.span_bert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed\n",
    "from ark_nlp.nn.layer.pooler_block import PoolerStartLogits, PoolerEndLogits\n",
    "from transformers import AutoModel, AutoModelForPreTraining, AutoTokenizer, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "tqdm.pandas(desc=\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_trans_to_C(string):\n",
    "    E_pun = u',.!?[]()<>\"\\''\n",
    "    C_pun = u'，。！？【】（）《》“‘'\n",
    "    table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "    return string.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\", sep=\"\\t\")\n",
    "train = pd.read_csv(\"data/train.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"text\"] = test[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"tag\"] = train[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 100%|██████████| 6000/6000 [00:00<00:00, 19546.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"entities\"] = train.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'label': entity_labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(datalist)\n",
    "train_data_df, dev_data_df = train_test_split(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 100%|██████████| 2628/2628 [00:00<00:00, 20239.83it/s]\n"
     ]
    }
   ],
   "source": [
    "tta = pd.read_csv(\"data/tta.csv\", sep=\"\\t\")\n",
    "tta[\"text\"] = tta[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "tta[\"tag\"] = tta[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])\n",
    "tta[\"entities\"] = tta.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)\n",
    "\n",
    "tta_datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    tta_datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'label': entity_labels\n",
    "    })\n",
    "\n",
    "tta_data = pd.DataFrame(tta_datalist)\n",
    "train_data_df = pd.concat([train_data_df, tta_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df.loc[:,['text', 'label']]\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "dev_data_df = dev_data_df.loc[:,['text', 'label']]\n",
    "dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset = Dataset(train_data_df)\n",
    "ner_dev_dataset = Dataset(dev_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab='roberta-base-finetuned-cluener2020-chinese', max_seq_len=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset.convert_to_ids(tokenizer)\n",
    "ner_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanDependenceBert(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    基于BERT指针的命名实体模型(end指针依赖start指针的结果)\n",
    "\n",
    "    Args:\n",
    "        config: 模型的配置对象\n",
    "        bert_trained (:obj:`bool`, optional): 预训练模型的参数是否可训练\n",
    "    \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        encoder_trained=True\n",
    "    ):\n",
    "        super(SpanDependenceBert, self).__init__(config)\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(\"./outputs/roberta-finetuned-cosine\")\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = encoder_trained\n",
    "\n",
    "        self.start_fc = PoolerStartLogits(config.hidden_size, self.num_labels)\n",
    "        self.end_fc = PoolerEndLogits(config.hidden_size + 1, self.num_labels)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True\n",
    "        ).hidden_states\n",
    "\n",
    "        sequence_output = outputs[-1]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        start_logits = self.start_fc(sequence_output)\n",
    "\n",
    "        label_logits = F.softmax(start_logits, -1)\n",
    "        label_logits = torch.argmax(label_logits, -1).unsqueeze(2).float()\n",
    "\n",
    "        end_logits = self.end_fc(sequence_output, label_logits)\n",
    "\n",
    "        return (start_logits, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SpanBertConfig.from_pretrained('./outputs/roberta-finetuned-cosine', num_labels=len(ner_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./outputs/roberta-finetuned-cosine were not used when initializing BertModel: ['encoder.weight_hh_l0', 'convLayer.convs.2.bias', 'encoder.weight_ih_l0_reverse', 'encoder.weight_hh_l0_reverse', 'predictor.linear.bias', 'encoder.bias_ih_l0', 'encoder.bias_hh_l0_reverse', 'predictor.mlp_rel.linear.weight', 'cln.weight', 'predictor.mlp2.linear.weight', 'encoder.weight_ih_l0', 'convLayer.base.1.weight', 'encoder.bias_hh_l0', 'reg_embs.weight', 'convLayer.convs.1.weight', 'predictor.mlp1.linear.bias', 'dis_embs.weight', 'cln.bias', 'convLayer.convs.2.weight', 'predictor.mlp_rel.linear.bias', 'convLayer.convs.0.weight', 'cln.bias_dense.weight', 'encoder.bias_ih_l0_reverse', 'convLayer.convs.1.bias', 'predictor.mlp1.linear.weight', 'predictor.linear.weight', 'predictor.mlp2.linear.bias', 'cln.weight_dense.weight', 'convLayer.base.1.bias', 'convLayer.convs.0.bias', 'predictor.biaffine.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ./outputs/roberta-finetuned-cosine were not used when initializing SpanDependenceBert: ['encoder.weight_hh_l0', 'convLayer.convs.2.bias', 'encoder.weight_ih_l0_reverse', 'encoder.weight_hh_l0_reverse', 'predictor.linear.bias', 'encoder.bias_ih_l0', 'encoder.bias_hh_l0_reverse', 'predictor.mlp_rel.linear.weight', 'cln.weight', 'predictor.mlp2.linear.weight', 'encoder.weight_ih_l0', 'convLayer.base.1.weight', 'encoder.bias_hh_l0', 'reg_embs.weight', 'convLayer.convs.1.weight', 'predictor.mlp1.linear.bias', 'dis_embs.weight', 'cln.bias', 'convLayer.convs.2.weight', 'predictor.mlp_rel.linear.bias', 'convLayer.convs.0.weight', 'cln.bias_dense.weight', 'encoder.bias_ih_l0_reverse', 'convLayer.convs.1.bias', 'predictor.mlp1.linear.weight', 'predictor.linear.weight', 'predictor.mlp2.linear.bias', 'cln.weight_dense.weight', 'convLayer.base.1.bias', 'convLayer.convs.0.bias', 'predictor.biaffine.weight']\n",
      "- This IS expected if you are initializing SpanDependenceBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpanDependenceBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpanDependenceBert were not initialized from the model checkpoint at ./outputs/roberta-finetuned-cosine and are newly initialized: ['start_fc.dense.weight', 'end_fc.LayerNorm.weight', 'end_fc.dense_1.weight', 'end_fc.LayerNorm.bias', 'end_fc.dense_0.weight', 'start_fc.dense.bias', 'end_fc.dense_0.bias', 'end_fc.dense_1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dl_module = SpanDependenceBert.from_pretrained('./outputs/roberta-finetuned-cosine', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches = 30\n",
    "batch_size = 256\n",
    "# 注意lr衰减轮次的设定\n",
    "show_step = len(ner_train_dataset) // batch_size + 2\n",
    "t_total = len(ner_train_dataset) // batch_size * num_epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_default_model_optimizer(dl_module, lr=1e-2, bert_lr=5e-5, weight_decay=0.01)\n",
    "scheduler = get_default_cosine_schedule_with_warmup(optimizer, t_total, warmup_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, 'ce', cude_device=2, scheduler=None, grad_clip=10.0, ema_decay=0.995, fgm_attack=True, save_path=\"outputs/roberta-finetuned-spanbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[0],train loss is:0.150960 \n",
      "\n",
      "eval_info:  {'acc': 0.786046511627907, 'recall': 0.8848167539267016, 'f1': 0.832512315270936}\n",
      "entity_info:  {'LOC': {'acc': 0.786, 'recall': 0.8848, 'f1': 0.8325}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:54<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[1],train loss is:0.027976 \n",
      "\n",
      "eval_info:  {'acc': 0.7467811158798283, 'recall': 0.9109947643979057, 'f1': 0.820754716981132}\n",
      "entity_info:  {'LOC': {'acc': 0.7468, 'recall': 0.911, 'f1': 0.8208}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[2],train loss is:0.017292 \n",
      "\n",
      "eval_info:  {'acc': 0.7927927927927928, 'recall': 0.9214659685863874, 'f1': 0.8523002421307506}\n",
      "entity_info:  {'LOC': {'acc': 0.7928, 'recall': 0.9215, 'f1': 0.8523}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[3],train loss is:0.015044 \n",
      "\n",
      "eval_info:  {'acc': 0.7990867579908676, 'recall': 0.9162303664921466, 'f1': 0.8536585365853657}\n",
      "entity_info:  {'LOC': {'acc': 0.7991, 'recall': 0.9162, 'f1': 0.8537}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[4],train loss is:0.012262 \n",
      "\n",
      "eval_info:  {'acc': 0.7807017543859649, 'recall': 0.9319371727748691, 'f1': 0.8496420047732697}\n",
      "entity_info:  {'LOC': {'acc': 0.7807, 'recall': 0.9319, 'f1': 0.8496}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:54<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[5],train loss is:0.009795 \n",
      "\n",
      "eval_info:  {'acc': 0.8301886792452831, 'recall': 0.9214659685863874, 'f1': 0.8734491315136477}\n",
      "entity_info:  {'LOC': {'acc': 0.8302, 'recall': 0.9215, 'f1': 0.8734}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[6],train loss is:0.009199 \n",
      "\n",
      "eval_info:  {'acc': 0.8130841121495327, 'recall': 0.9109947643979057, 'f1': 0.8592592592592593}\n",
      "entity_info:  {'LOC': {'acc': 0.8131, 'recall': 0.911, 'f1': 0.8593}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[7],train loss is:0.007964 \n",
      "\n",
      "eval_info:  {'acc': 0.8110599078341014, 'recall': 0.9214659685863874, 'f1': 0.8627450980392156}\n",
      "entity_info:  {'LOC': {'acc': 0.8111, 'recall': 0.9215, 'f1': 0.8627}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[8],train loss is:0.007751 \n",
      "\n",
      "eval_info:  {'acc': 0.7882882882882883, 'recall': 0.9162303664921466, 'f1': 0.847457627118644}\n",
      "entity_info:  {'LOC': {'acc': 0.7883, 'recall': 0.9162, 'f1': 0.8475}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[9],train loss is:0.007667 \n",
      "\n",
      "eval_info:  {'acc': 0.8148148148148148, 'recall': 0.9214659685863874, 'f1': 0.8648648648648648}\n",
      "entity_info:  {'LOC': {'acc': 0.8148, 'recall': 0.9215, 'f1': 0.8649}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[10],train loss is:0.007375 \n",
      "\n",
      "eval_info:  {'acc': 0.7918552036199095, 'recall': 0.9162303664921466, 'f1': 0.8495145631067962}\n",
      "entity_info:  {'LOC': {'acc': 0.7919, 'recall': 0.9162, 'f1': 0.8495}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[11],train loss is:0.006950 \n",
      "\n",
      "eval_info:  {'acc': 0.7873303167420814, 'recall': 0.9109947643979057, 'f1': 0.8446601941747572}\n",
      "entity_info:  {'LOC': {'acc': 0.7873, 'recall': 0.911, 'f1': 0.8447}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[12],train loss is:0.006558 \n",
      "\n",
      "eval_info:  {'acc': 0.7797356828193832, 'recall': 0.9267015706806283, 'f1': 0.8468899521531101}\n",
      "entity_info:  {'LOC': {'acc': 0.7797, 'recall': 0.9267, 'f1': 0.8469}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[13],train loss is:0.006125 \n",
      "\n",
      "eval_info:  {'acc': 0.7719298245614035, 'recall': 0.9214659685863874, 'f1': 0.8400954653937949}\n",
      "entity_info:  {'LOC': {'acc': 0.7719, 'recall': 0.9215, 'f1': 0.8401}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[14],train loss is:0.006061 \n",
      "\n",
      "eval_info:  {'acc': 0.7729257641921398, 'recall': 0.9267015706806283, 'f1': 0.8428571428571427}\n",
      "entity_info:  {'LOC': {'acc': 0.7729, 'recall': 0.9267, 'f1': 0.8429}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[15],train loss is:0.006453 \n",
      "\n",
      "eval_info:  {'acc': 0.7629310344827587, 'recall': 0.9267015706806283, 'f1': 0.8368794326241136}\n",
      "entity_info:  {'LOC': {'acc': 0.7629, 'recall': 0.9267, 'f1': 0.8369}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[16],train loss is:0.007121 \n",
      "\n",
      "eval_info:  {'acc': 0.7927927927927928, 'recall': 0.9214659685863874, 'f1': 0.8523002421307506}\n",
      "entity_info:  {'LOC': {'acc': 0.7928, 'recall': 0.9215, 'f1': 0.8523}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[17],train loss is:0.006414 \n",
      "\n",
      "eval_info:  {'acc': 0.7857142857142857, 'recall': 0.9214659685863874, 'f1': 0.8481927710843373}\n",
      "entity_info:  {'LOC': {'acc': 0.7857, 'recall': 0.9215, 'f1': 0.8482}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[18],train loss is:0.006651 \n",
      "\n",
      "eval_info:  {'acc': 0.7847533632286996, 'recall': 0.9162303664921466, 'f1': 0.8454106280193238}\n",
      "entity_info:  {'LOC': {'acc': 0.7848, 'recall': 0.9162, 'f1': 0.8454}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:54<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[19],train loss is:0.006041 \n",
      "\n",
      "eval_info:  {'acc': 0.7847533632286996, 'recall': 0.9162303664921466, 'f1': 0.8454106280193238}\n",
      "entity_info:  {'LOC': {'acc': 0.7848, 'recall': 0.9162, 'f1': 0.8454}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[20],train loss is:0.005669 \n",
      "\n",
      "eval_info:  {'acc': 0.7822222222222223, 'recall': 0.9214659685863874, 'f1': 0.8461538461538461}\n",
      "entity_info:  {'LOC': {'acc': 0.7822, 'recall': 0.9215, 'f1': 0.8462}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[21],train loss is:0.005656 \n",
      "\n",
      "eval_info:  {'acc': 0.7937219730941704, 'recall': 0.9267015706806283, 'f1': 0.8550724637681159}\n",
      "entity_info:  {'LOC': {'acc': 0.7937, 'recall': 0.9267, 'f1': 0.8551}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[22],train loss is:0.005401 \n",
      "\n",
      "eval_info:  {'acc': 0.8119266055045872, 'recall': 0.9267015706806283, 'f1': 0.8655256723716381}\n",
      "entity_info:  {'LOC': {'acc': 0.8119, 'recall': 0.9267, 'f1': 0.8655}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[23],train loss is:0.005581 \n",
      "\n",
      "eval_info:  {'acc': 0.8036529680365296, 'recall': 0.9214659685863874, 'f1': 0.8585365853658535}\n",
      "entity_info:  {'LOC': {'acc': 0.8037, 'recall': 0.9215, 'f1': 0.8585}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[24],train loss is:0.005568 \n",
      "\n",
      "eval_info:  {'acc': 0.8165137614678899, 'recall': 0.9319371727748691, 'f1': 0.8704156479217604}\n",
      "entity_info:  {'LOC': {'acc': 0.8165, 'recall': 0.9319, 'f1': 0.8704}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[25],train loss is:0.005165 \n",
      "\n",
      "eval_info:  {'acc': 0.8202764976958525, 'recall': 0.9319371727748691, 'f1': 0.8725490196078431}\n",
      "entity_info:  {'LOC': {'acc': 0.8203, 'recall': 0.9319, 'f1': 0.8725}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[26],train loss is:0.005118 \n",
      "\n",
      "eval_info:  {'acc': 0.7946428571428571, 'recall': 0.9319371727748691, 'f1': 0.8578313253012048}\n",
      "entity_info:  {'LOC': {'acc': 0.7946, 'recall': 0.9319, 'f1': 0.8578}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[27],train loss is:0.004777 \n",
      "\n",
      "eval_info:  {'acc': 0.8, 'recall': 0.9214659685863874, 'f1': 0.856447688564477}\n",
      "entity_info:  {'LOC': {'acc': 0.8, 'recall': 0.9215, 'f1': 0.8564}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[28],train loss is:0.005030 \n",
      "\n",
      "eval_info:  {'acc': 0.7652173913043478, 'recall': 0.9214659685863874, 'f1': 0.8361045130641329}\n",
      "entity_info:  {'LOC': {'acc': 0.7652, 'recall': 0.9215, 'f1': 0.8361}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[29],train loss is:0.005025 \n",
      "\n",
      "eval_info:  {'acc': 0.7619047619047619, 'recall': 0.9214659685863874, 'f1': 0.834123222748815}\n",
      "entity_info:  {'LOC': {'acc': 0.7619, 'recall': 0.9215, 'f1': 0.8341}}\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    ner_train_dataset,\n",
    "    ner_dev_dataset,\n",
    "    lr=2e-4,\n",
    "    epochs=num_epoches,\n",
    "    batch_size=batch_size,\n",
    "    show_step=show_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ark_nlp.model.ner.span_bert' from '/data/lpzhang/ner/ark_nlp/model/ner/span_bert/__init__.py'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ark_nlp.model.ner.span_bert as span\n",
    "import imp\n",
    "imp.reload(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ark_nlp.model.ner.span_bert import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_predictor_instance = span.Predictor(model.module, tokenizer, ner_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2657/2657 [00:25<00:00, 104.58it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_results = []\n",
    "tta_data = []\n",
    "\n",
    "for _line in tqdm(test[\"text\"].tolist()):\n",
    "    label = set()\n",
    "    for _preditc in ner_predictor_instance.predict_one_sample(_line):\n",
    "        label.add(_preditc[\"entity\"][:-1])\n",
    "    \n",
    "    label = list(label)\n",
    "    if len(label) > 0:\n",
    "        tta_data.append([_line, label])\n",
    "\n",
    "    predict_results.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spanbert_submit.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"tag\\n\")\n",
    "    for _result in predict_results:\n",
    "       f.write(f\"{str(_result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tta_data = pd.DataFrame(tta_data, columns=[\"text\", \"tag\"])\n",
    "# tta_data.to_csv(\"data/tta.csv\", index=False, encoding=\"utf-8\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91d8ac8ac98a00daee01b170ecc4de38a4b78e57473b1984dedfa9b67acb5aae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
