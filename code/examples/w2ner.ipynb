{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ark_nlp.model.ner.w2ner_bert import W2NERBert\n",
    "from ark_nlp.model.ner.w2ner_bert import W2NERBertConfig\n",
    "from ark_nlp.model.ner.w2ner_bert import Dataset\n",
    "from ark_nlp.model.ner.w2ner_bert import Task\n",
    "from ark_nlp.model.ner.w2ner_bert import get_default_w2ner_optimizer\n",
    "from ark_nlp.factory.lr_scheduler import get_default_linear_schedule_with_warmup, get_default_cosine_schedule_with_warmup\n",
    "from ark_nlp.model.ner.w2ner_bert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed\n",
    "\n",
    "set_seed(2022)\n",
    "tqdm.pandas(desc=\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_trans_to_C(string):\n",
    "    E_pun = u',.!?[]()<>\"\\''\n",
    "    C_pun = u'，。！？【】（）《》“‘'\n",
    "    table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "    return string.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(\"data/test.csv\", sep=\"\\t\")\n",
    "train = pd.read_csv(\"data/train.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[\"text\"] = test[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"tag\"] = train[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 100%|██████████| 6000/6000 [00:00<00:00, 19502.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"entities\"] = train.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'entities': entity_labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(datalist)\n",
    "train_data_df, dev_data_df = train_test_split(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 100%|██████████| 2628/2628 [00:00<00:00, 17030.31it/s]\n"
     ]
    }
   ],
   "source": [
    "pseudo = pd.read_csv(\"submits/data/pseudo_best.csv\", sep=\"\\t\")\n",
    "pseudo[\"text\"] = pseudo[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "pseudo[\"tag\"] = pseudo[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])\n",
    "pseudo = pseudo[pseudo[\"tag\"].apply(len) > 0]\n",
    "pseudo[\"entities\"] = pseudo.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)\n",
    "\n",
    "pseudo_datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    pseudo_datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'entities': entity_labels\n",
    "    })\n",
    "\n",
    "pseudo_data = pd.DataFrame(pseudo_datalist)\n",
    "train_data_df = pd.concat([train_data_df, pseudo_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(x):\n",
    "    \n",
    "    entities = []\n",
    "    for entity in x:\n",
    "        entity_ = {}\n",
    "        idx = list(range(entity['start_idx'], entity['end_idx']))\n",
    "        entity_['idx'] = idx\n",
    "        entity_['type'] = entity['type']\n",
    "        entity_['entity'] = entity['entity']\n",
    "        entities.append(entity_)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['label'] = train_data_df['entities'].apply(lambda x: get_label(x))\n",
    "dev_data_df['label'] = dev_data_df['entities'].apply(lambda x: get_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df.loc[:,['text', 'label']]\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "dev_data_df = dev_data_df.loc[:,['text', 'label']]\n",
    "dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset = Dataset(train_data_df)\n",
    "ner_dev_dataset = Dataset(dev_data_df, categories=ner_train_dataset.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = None\n",
    "tokenizer = Tokenizer(vocab='chinese-roberta-large-upos', max_seq_len=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset.convert_to_ids(tokenizer, prompt=prompt)\n",
    "ner_dev_dataset.convert_to_ids(tokenizer, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"提取上述句子中的所有命名实体信息\"\n",
    "# tokenizer = Tokenizer(vocab='roberta-base-finetuned-cluener2020-chinese', max_seq_len=52 + len(prompt) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_train_dataset.convert_to_ids(tokenizer, prompt=prompt)\n",
    "# ner_dev_dataset.convert_to_ids(tokenizer, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = W2NERBertConfig.from_pretrained('chinese-roberta-large-upos', num_labels=len(ner_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at chinese-roberta-large-upos were not used when initializing W2NERBert: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing W2NERBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing W2NERBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of W2NERBert were not initialized from the model checkpoint at chinese-roberta-large-upos and are newly initialized: ['convLayer.convs.0.bias', 'bert.pooler.dense.bias', 'predictor.mlp1.linear.bias', 'predictor.mlp2.linear.bias', 'predictor.mlp_rel.linear.bias', 'convLayer.convs.1.weight', 'encoder.bias_ih_l0_reverse', 'cln.gamma', 'predictor.linear.bias', 'convLayer.convs.2.bias', 'predictor.mlp2.linear.weight', 'cln.beta', 'reg_embs.weight', 'cln.beta_dense.weight', 'cln.gamma_dense.weight', 'encoder.bias_hh_l0', 'convLayer.base.1.weight', 'encoder.bias_hh_l0_reverse', 'bert.pooler.dense.weight', 'predictor.biaffine.weight', 'predictor.mlp_rel.linear.weight', 'convLayer.convs.0.weight', 'predictor.linear.weight', 'encoder.weight_hh_l0_reverse', 'encoder.weight_ih_l0', 'encoder.weight_ih_l0_reverse', 'predictor.mlp1.linear.weight', 'convLayer.base.1.bias', 'encoder.bias_ih_l0', 'convLayer.convs.2.weight', 'dis_embs.weight', 'convLayer.convs.1.bias', 'encoder.weight_hh_l0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dl_module = W2NERBert.from_pretrained('chinese-roberta-large-upos', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches, batch_size = 10, 16 # 0.91\n",
    "# num_epoches, batch_size = 15, 16\n",
    "# num_epoches, batch_size = 40, 256 # 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = get_default_w2ner_optimizer(dl_module) # 0.91\n",
    "# # optimizer = get_default_w2ner_optimizer(dl_module, lr=5e-4, bert_lr=1e-5, weight_decay=0.01)\n",
    "# # optimizer = get_default_w2ner_optimizer(dl_module, lr=5e-2, bert_lr=5e-5, weight_decay=0.01)\n",
    "optimizer = get_default_w2ner_optimizer(dl_module, lr=1e-2, bert_lr=5e-5, weight_decay=0.01) # 0.91143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意lr衰减轮次的设定\n",
    "show_step = len(ner_train_dataset) // batch_size + 2\n",
    "t_total = len(ner_train_dataset) // batch_size * num_epoches\n",
    "scheduler = get_default_cosine_schedule_with_warmup(optimizer, t_total, warmup_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Task(dl_module, optimizer, 'ce', cude_device=2, scheduler=scheduler, grad_clip=5.0, ema_decay=0.995, save_path=\"outputs/roberta-finetuned-large\") # 0.91\n",
    "model = Task(dl_module, optimizer, 'ce', cude_device=2, scheduler=scheduler, grad_clip=5.0, ema_decay=0.995, fgm_attack=True, save_path=\"outputs/roberta-finetuned-prompt\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 638/638 [04:32<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[0],train loss is:0.041481 \n",
      "\n",
      "eval loss is 0.003307, precision is:0.8037578288100209, recall is:0.8621241202815099, f1_score is:0.831918505942275\n",
      "current best metric: 0.8621241202815099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 638/638 [04:32<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[1],train loss is:0.003001 \n",
      "\n",
      "save best model to outputs/roberta-finetuned-promot_best.\n",
      "eval loss is 0.001973, precision is:0.8744257274119449, recall is:0.9133077415227128, f1_score is:0.8934439054920984\n",
      "current best metric: 0.9133077415227128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 638/638 [04:32<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[2],train loss is:0.001862 \n",
      "\n",
      "save best model to outputs/roberta-finetuned-promot_best.\n",
      "eval loss is 0.001271, precision is:0.9225, recall is:0.944337811900192, f1_score is:0.9332911792601961\n",
      "current best metric: 0.944337811900192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 638/638 [04:33<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[3],train loss is:0.001280 \n",
      "\n",
      "save best model to outputs/roberta-finetuned-promot_best.\n",
      "eval loss is 0.000902, precision is:0.94976, recall is:0.9494561740243123, f1_score is:0.9496080627099664\n",
      "current best metric: 0.9494561740243123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 638/638 [04:32<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[4],train loss is:0.000878 \n",
      "\n",
      "save best model to outputs/roberta-finetuned-promot_best.\n",
      "eval loss is 0.000681, precision is:0.9689846555664381, recall is:0.9494561740243123, f1_score is:0.9591210211665859\n",
      "current best metric: 0.9494561740243123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 270/638 [01:55<02:38,  2.32it/s]"
     ]
    }
   ],
   "source": [
    "# model.fit(ner_train_dataset, ner_dev_dataset, lr=5e-5, epochs=num_epoches, batch_size=batch_size, show_step=show_step)\n",
    "model.fit(ner_train_dataset, ner_dev_dataset, epochs=num_epoches, batch_size=batch_size, show_step=show_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.model.ner.w2ner_bert import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFW2NERPredictor(Predictor):\n",
    "    def E_trans_to_C(self, string):\n",
    "        E_pun = u',.!?[]()<>\"\\''\n",
    "        C_pun = u'，。！？【】（）《》“‘'\n",
    "        table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "\n",
    "        return string.translate(table)\n",
    "\n",
    "    def predict_one_sample(self, text='', prompt=None, cv=False):\n",
    "        text = text.strip()\n",
    "        \n",
    "        features = self._get_input_ids(E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", text)), prompt=prompt)\n",
    "        self.module.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = self._get_module_one_sample_inputs(features)\n",
    "            logit = self.module(**inputs)\n",
    "\n",
    "        preds = torch.argmax(logit, -1)\n",
    "\n",
    "        instance, l = preds.cpu().numpy()[0], int(inputs['input_lengths'].cpu().numpy()[0])\n",
    "\n",
    "        forward_dict = {}\n",
    "        head_dict = {}\n",
    "        ht_type_dict = {}\n",
    "        for i in range(l):\n",
    "            for j in range(i + 1, l):\n",
    "                if instance[i, j] == 1:\n",
    "                    if i not in forward_dict:\n",
    "                        forward_dict[i] = [j]\n",
    "                    else:\n",
    "                        forward_dict[i].append(j)\n",
    "        for i in range(l):\n",
    "            for j in range(i, l):\n",
    "                if instance[j, i] > 1:\n",
    "                    ht_type_dict[(i, j)] = instance[j, i]\n",
    "                    if i not in head_dict:\n",
    "                        head_dict[i] = {j}\n",
    "                    else:\n",
    "                        head_dict[i].add(j)\n",
    "\n",
    "        predicts = []\n",
    "\n",
    "        def find_entity(key, entity, tails):\n",
    "            entity.append(key)\n",
    "            if key not in forward_dict:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "                entity.pop()\n",
    "                return\n",
    "            else:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "            for k in forward_dict[key]:\n",
    "                find_entity(k, entity, tails)\n",
    "            entity.pop()\n",
    "\n",
    "        for head in head_dict:\n",
    "            find_entity(head, [], head_dict[head])\n",
    "\n",
    "        entities = []\n",
    "        for entity_ in predicts:\n",
    "            entities.append({\n",
    "                \"idx\": entity_,\n",
    "                \"entity\": ''.join([text[i] for i in entity_]),\n",
    "                \"type\": self.id2cat[ht_type_dict[(entity_[0], entity_[-1])]]\n",
    "            })\n",
    "\n",
    "        if cv:\n",
    "            return text, int(inputs['input_lengths'].cpu().numpy()[0]), logit.cpu().numpy()\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def get_result(self, text, text_len, logit):\n",
    "        preds = np.argmax(logit, -1)\n",
    "\n",
    "        instance, l = preds[0], text_len\n",
    "\n",
    "        forward_dict = {}\n",
    "        head_dict = {}\n",
    "        ht_type_dict = {}\n",
    "        for i in range(l):\n",
    "            for j in range(i + 1, l):\n",
    "                if instance[i, j] == 1:\n",
    "                    if i not in forward_dict:\n",
    "                        forward_dict[i] = [j]\n",
    "                    else:\n",
    "                        forward_dict[i].append(j)\n",
    "        for i in range(l):\n",
    "            for j in range(i, l):\n",
    "                if instance[j, i] > 1:\n",
    "                    ht_type_dict[(i, j)] = instance[j, i]\n",
    "                    if i not in head_dict:\n",
    "                        head_dict[i] = {j}\n",
    "                    else:\n",
    "                        head_dict[i].add(j)\n",
    "\n",
    "        predicts = []\n",
    "\n",
    "        def find_entity(key, entity, tails):\n",
    "            entity.append(key)\n",
    "            if key not in forward_dict:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "                entity.pop()\n",
    "                return\n",
    "            else:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "            for k in forward_dict[key]:\n",
    "                find_entity(k, entity, tails)\n",
    "            entity.pop()\n",
    "\n",
    "        for head in head_dict:\n",
    "            find_entity(head, [], head_dict[head])\n",
    "\n",
    "        entities = []\n",
    "        for entity_ in predicts:\n",
    "            entities.append({\n",
    "                \"idx\": entity_,\n",
    "                \"entity\": ''.join([text[i] for i in entity_]),\n",
    "                \"type\": self.id2cat[ht_type_dict[(entity_[0], entity_[-1])]]\n",
    "            })\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_predictor_instance = IFW2NERPredictor(model.module, tokenizer, ner_train_dataset.cat2id)\n",
    "ner_predictor_instance_best = IFW2NERPredictor(torch.load(\"outputs/roberta-finetuned-prompt_best.pkl\"), tokenizer, ner_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = []\n",
    "tta_data = []\n",
    "\n",
    "for _line in tqdm(test[\"text\"].tolist()):\n",
    "    label = set()\n",
    "\n",
    "    text, text_len, logit = ner_predictor_instance.predict_one_sample(_line, prompt=prompt, cv=True)\n",
    "    _, _, logit_best = ner_predictor_instance_best.predict_one_sample(_line, prompt=prompt, cv=True)\n",
    "\n",
    "    logit = np.sum(np.array([logit_best, logit]), axis=0)\n",
    "\n",
    "    for _preditc in ner_predictor_instance.get_result(text, text_len, logit):\n",
    "        label.add(_preditc[\"entity\"])\n",
    "    \n",
    "    label = list(label)\n",
    "    if len(label) > 0:\n",
    "        tta_data.append([_line, label])\n",
    "\n",
    "    predict_results.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2ner_submit_prompt_cv.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"tag\\n\")\n",
    "    for _result in predict_results:\n",
    "       f.write(f\"{str(_result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_predictor_instance = IFW2NERPredictor(torch.load(\"outputs/roberta-finetuned.pkl\"), tokenizer, ner_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = []\n",
    "\n",
    "for _line in tqdm(test[\"text\"].tolist()):\n",
    "    label = set()\n",
    "    for _preditc in ner_predictor_instance.predict_one_sample(_line, prompt=prompt):\n",
    "        label.add(_preditc[\"entity\"])\n",
    "    label = list(label)\n",
    "\n",
    "    predict_results.append(label)\n",
    "\n",
    "with open('w2ner_submit_prompt.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"tag\\n\")\n",
    "    for _result in predict_results:\n",
    "       f.write(f\"{str(_result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo_data = pd.DataFrame(pseudo_data, columns=[\"text\", \"tag\"])\n",
    "# pseudo_data.to_csv(\"data/pseudo.csv\", index=False, encoding=\"utf-8\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# model.module.save_pretrained(\"outputs/roberta-finetuned\")\n",
    "# AutoTokenizer.from_pretrained('roberta-base-finetuned-cluener2020-chinese').save_pretrained(\"outputs/roberta-finetuned-cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
