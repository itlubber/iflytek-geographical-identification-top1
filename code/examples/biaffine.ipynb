{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ark_nlp.model.ner.biaffine_bert import BiaffineBert\n",
    "from ark_nlp.model.ner.biaffine_bert import BiaffineBertConfig\n",
    "from ark_nlp.model.ner.biaffine_bert import Dataset\n",
    "from ark_nlp.model.ner.biaffine_bert import Task\n",
    "from ark_nlp.model.ner.biaffine_bert import get_default_model_optimizer\n",
    "from ark_nlp.factory.optimizer import get_w2ner_model_optimizer as get_biaffine_model_optimizer\n",
    "from ark_nlp.factory.lr_scheduler import get_default_cosine_schedule_with_warmup\n",
    "from ark_nlp.model.ner.biaffine_bert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed\n",
    "from ark_nlp.nn.layer.biaffine_block import Biaffine\n",
    "from transformers import AutoModel, AutoModelForPreTraining, AutoTokenizer, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas(desc=\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_trans_to_C(string):\n",
    "    E_pun = u',.!?[]()<>\"\\''\n",
    "    C_pun = u'，。！？【】（）《》“‘'\n",
    "    table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "    return string.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\", sep=\"\\t\")\n",
    "train = pd.read_csv(\"data/train.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"text\"] = test[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"tag\"] = train[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"entities\"] = train.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'label': entity_labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(datalist)\n",
    "train_data_df, dev_data_df = train_test_split(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo = pd.read_csv(\"data/pseudo.csv\", sep=\"\\t\")\n",
    "pseudo[\"text\"] = pseudo[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "pseudo[\"tag\"] = pseudo[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])\n",
    "pseudo[\"entities\"] = pseudo.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)\n",
    "\n",
    "pseudo_datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    pseudo_datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'label': entity_labels\n",
    "    })\n",
    "\n",
    "pseudo_data = pd.DataFrame(pseudo_datalist)\n",
    "train_data_df = pd.concat([train_data_df, pseudo_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df.loc[:,['text', 'label']]\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "dev_data_df = dev_data_df.loc[:,['text', 'label']]\n",
    "dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset = Dataset(train_data_df)\n",
    "ner_dev_dataset = Dataset(dev_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab='roberta-base-finetuned-cluener2020-chinese', max_seq_len=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset.convert_to_ids(tokenizer)\n",
    "ner_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiaffineBert(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        encoder_trained=True,\n",
    "        biaffine_size=128,\n",
    "        lstm_dropout=0.4,\n",
    "        select_bert_layer=-1\n",
    "    ):\n",
    "        super(BiaffineBert, self).__init__(config)\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "        self.select_bert_layer = select_bert_layer\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(\"./outputs/roberta-finetuned-cosine\")\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = encoder_trained\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=config.hidden_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=lstm_dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.start_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(\n",
    "                in_features=2*config.hidden_size,\n",
    "                out_features=biaffine_size),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.end_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(\n",
    "                in_features=2*config.hidden_size,\n",
    "                out_features=biaffine_size),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.biaffne = Biaffine(biaffine_size, self.num_labels)\n",
    "\n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        nn.init.xavier_uniform_(self.start_encoder[0].weight)\n",
    "        nn.init.xavier_uniform_(self.end_encoder[0].weight)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs.hidden_states[self.select_bert_layer]\n",
    "\n",
    "        # lstm编码\n",
    "        sequence_output, _ = self.lstm(sequence_output)\n",
    "\n",
    "        start_logits = self.start_encoder(sequence_output)\n",
    "        end_logits = self.end_encoder(sequence_output)\n",
    "\n",
    "        span_logits = self.biaffne(start_logits, end_logits)\n",
    "        span_logits = span_logits.contiguous()\n",
    "\n",
    "        return span_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BiaffineBertConfig.from_pretrained('./outputs/roberta-finetuned-cosine', num_labels=len(ner_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_module = BiaffineBert.from_pretrained('./outputs/roberta-finetuned-cosine', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches = 30\n",
    "batch_size = 256\n",
    "# 注意lr衰减轮次的设定\n",
    "show_step = len(ner_train_dataset) // batch_size + 2\n",
    "t_total = len(ner_train_dataset) // batch_size * num_epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = get_biaffine_model_optimizer(dl_module, lr=5e-4, bert_lr=1e-5, weight_decay=0.01)\n",
    "optimizer = get_default_model_optimizer(dl_module)\n",
    "scheduler = get_default_cosine_schedule_with_warmup(optimizer, t_total, warmup_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, torch.nn.CrossEntropyLoss(reduction=\"none\"), scheduler=None, cude_device=2, grad_clip=10.0, ema_decay=0.995, fgm_attack=True, save_path=\"outputs/roberta-finetuned-biaffine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ner_train_dataset,\n",
    "    ner_dev_dataset,\n",
    "    lr=2e-4,\n",
    "    epochs=num_epoches,\n",
    "    batch_size=batch_size,\n",
    "    show_step=show_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ark_nlp.model.ner.biaffine_bert as biaffine\n",
    "import imp\n",
    "imp.reload(biaffine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_predictor_instance = biaffine.Predictor(model.module, tokenizer, ner_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = []\n",
    "pseudo_data = []\n",
    "\n",
    "for _line in tqdm(test[\"text\"].tolist()):\n",
    "    label = set()\n",
    "    for _preditc in ner_predictor_instance.predict_one_sample(_line):\n",
    "        label.add(_preditc[\"entity\"][:-1])\n",
    "    \n",
    "    label = list(label)\n",
    "    if len(label) > 0:\n",
    "        pseudo_data.append([_line, label])\n",
    "\n",
    "    predict_results.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('biaffine_submit.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"tag\\n\")\n",
    "    for _result in predict_results:\n",
    "       f.write(f\"{str(_result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo_data = pd.DataFrame(pseudo_data, columns=[\"text\", \"tag\"])\n",
    "# pseudo_data.to_csv(\"data/pseudo.csv\", index=False, encoding=\"utf-8\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91d8ac8ac98a00daee01b170ecc4de38a4b78e57473b1984dedfa9b67acb5aae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
