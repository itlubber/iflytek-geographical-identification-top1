{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from ark_nlp.model.ner.w2ner_bert import W2NERBert\n",
    "from ark_nlp.model.ner.w2ner_bert import W2NERBertConfig\n",
    "from ark_nlp.model.ner.w2ner_bert import Dataset\n",
    "from ark_nlp.model.ner.w2ner_bert import Task\n",
    "from ark_nlp.model.ner.w2ner_bert import get_default_w2ner_optimizer\n",
    "from ark_nlp.factory.lr_scheduler import get_default_linear_schedule_with_warmup, get_default_cosine_schedule_with_warmup\n",
    "from ark_nlp.model.ner.w2ner_bert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "tqdm.pandas(desc=\"inference\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_trans_to_C(string):\n",
    "    E_pun = u',.!?[]()<>\"\\''\n",
    "    C_pun = u'，。！？【】（）《》“‘'\n",
    "    table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "    return string.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(\"data/test.csv\", sep=\"\\t\")\n",
    "train = pd.read_csv(\"data/train.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[\"text\"] = test[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"tag\"] = train[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 100%|██████████| 6000/6000 [00:00<00:00, 19458.15it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"entities\"] = train.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'entities': entity_labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 100%|██████████| 2628/2628 [00:00<00:00, 20023.93it/s]\n"
     ]
    }
   ],
   "source": [
    "pseudo = pd.read_csv(\"submits/data/pseudo_best.csv\", sep=\"\\t\")\n",
    "pseudo[\"text\"] = pseudo[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "pseudo[\"tag\"] = pseudo[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])\n",
    "pseudo = pseudo[pseudo[\"tag\"].apply(len) > 0]\n",
    "pseudo[\"entities\"] = pseudo.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)\n",
    "\n",
    "pseudo_datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    pseudo_datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'entities': entity_labels\n",
    "    })\n",
    "\n",
    "pseudo_data = pd.DataFrame(pseudo_datalist)\n",
    "dataset = pd.concat([data, pseudo_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(x):\n",
    "    \n",
    "    entities = []\n",
    "    for entity in x:\n",
    "        if entity['entity'].strip():\n",
    "            entity_ = {}\n",
    "            idx = list(range(entity['start_idx'], entity['end_idx']))\n",
    "            entity_['idx'] = idx\n",
    "            entity_['type'] = entity['type']\n",
    "            entity_['entity'] = entity['entity']\n",
    "            entities.append(entity_)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['label'] = dataset['entities'].apply(lambda x: get_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.loc[:,['text', 'label']]\n",
    "dataset['label'] = dataset['label'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "seed = 42\n",
    "device = 1\n",
    "max_len = 52\n",
    "batch_size = 128\n",
    "num_epoches = 20\n",
    "model_name = \"roberta-base-finetuned-cluener2020-chinese\"\n",
    "categories = ['<none>', '<suc>', 'LOC']\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizer\n",
    "from ark_nlp.nn.layer.nezha_block import NeZhaPreTrainedModel, NeZhaModel\n",
    "from ark_nlp.nn.configuration.configuration_nezha import NeZhaConfig\n",
    "from ark_nlp.nn.layer.roformer_block import RoFormerPreTrainedModel\n",
    "from ark_nlp.nn.configuration.configuration_roformer import RoFormerConfig\n",
    "from ark_nlp.model.ner.w2ner_bert.w2ner_bert import ConvolutionLayer, CoPredictor, LayerNorm, pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2NERBert(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        use_bert_last_4_layers=True,\n",
    "\n",
    "        dist_emb_size=20,\n",
    "        type_emb_size=20,\n",
    "        lstm_hid_size=512,\n",
    "        conv_hid_size=96,\n",
    "\n",
    "        biaffine_size=512,\n",
    "        ffnn_hid_size=288,\n",
    "        dilation=[1, 2, 3],\n",
    "\n",
    "        conv_dropout=0.5,\n",
    "        emb_dropout=0.5,\n",
    "        out_dropout=0.33,\n",
    "        ** kwargs\n",
    "    ):\n",
    "        super(W2NERBert, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.use_bert_last_4_layers = use_bert_last_4_layers\n",
    "\n",
    "        self.lstm_hid_size = lstm_hid_size\n",
    "        self.conv_hid_size = conv_hid_size\n",
    "\n",
    "        lstm_input_size = 0\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        lstm_input_size += config.hidden_size\n",
    "\n",
    "        self.dis_embs = nn.Embedding(20, dist_emb_size)\n",
    "        self.reg_embs = nn.Embedding(3, type_emb_size)\n",
    "\n",
    "        self.encoder = nn.LSTM(lstm_input_size, lstm_hid_size // 2, num_layers=1, batch_first=True,\n",
    "                               bidirectional=True)\n",
    "\n",
    "        conv_input_size = lstm_hid_size + dist_emb_size + type_emb_size\n",
    "\n",
    "        self.convLayer = ConvolutionLayer(conv_input_size, conv_hid_size, dilation, conv_dropout)\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.predictor = CoPredictor(self.num_labels, lstm_hid_size, biaffine_size,\n",
    "                                     conv_hid_size * len(dilation), ffnn_hid_size,\n",
    "                                     out_dropout)\n",
    "\n",
    "        self.cln = LayerNorm(lstm_hid_size, lstm_hid_size, conditional=True)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            grid_mask2d,\n",
    "            dist_inputs,\n",
    "            pieces2word,\n",
    "            input_lengths,\n",
    "            **kwargs\n",
    "    ):\n",
    "        bert_embs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True,)\n",
    "\n",
    "        if self.use_bert_last_4_layers:\n",
    "            bert_embs = torch.stack(bert_embs[2][-4:], dim=-1).mean(-1)\n",
    "        else:\n",
    "            bert_embs = bert_embs[0]\n",
    "\n",
    "        length = pieces2word.size(1)\n",
    "\n",
    "        min_value = torch.min(bert_embs).item()\n",
    "\n",
    "        # Max pooling word representations from pieces\n",
    "        _bert_embs = bert_embs.unsqueeze(1).expand(-1, length, -1, -1)\n",
    "        _bert_embs = torch.masked_fill(_bert_embs, pieces2word.eq(0).unsqueeze(-1), min_value)\n",
    "        word_reps, _ = torch.max(_bert_embs, dim=2)\n",
    "\n",
    "        word_reps = self.dropout(word_reps)\n",
    "        packed_embs = pack_padded_sequence(word_reps, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_outs, (hidden, _) = self.encoder(packed_embs)\n",
    "        # 源码每个batch的长度等于 input_lengths.max()\n",
    "        word_reps, _ = pad_packed_sequence(packed_outs, batch_first=True, total_length=length)\n",
    "\n",
    "        cln = self.cln(word_reps.unsqueeze(2), word_reps)\n",
    "\n",
    "        dis_emb = self.dis_embs(dist_inputs)\n",
    "        tril_mask = torch.tril(grid_mask2d.clone().long())\n",
    "        reg_inputs = tril_mask + grid_mask2d.clone().long()\n",
    "        reg_emb = self.reg_embs(reg_inputs)\n",
    "\n",
    "        conv_inputs = torch.cat([dis_emb, reg_emb, cln], dim=-1)\n",
    "        conv_inputs = torch.masked_fill(conv_inputs, grid_mask2d.eq(0).unsqueeze(-1), 0.0)\n",
    "        conv_outputs = self.convLayer(conv_inputs)\n",
    "        conv_outputs = torch.masked_fill(conv_outputs, grid_mask2d.eq(0).unsqueeze(-1), 0.0)\n",
    "        outputs = self.predictor(word_reps, word_reps, conv_outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2NERNezha(NeZhaPreTrainedModel):\n",
    "\n",
    "    def __init__(self,use_bert_last_4_layers=True,dist_emb_size=20,type_emb_size=20,lstm_hid_size=512,conv_hid_size=96,biaffine_size=512,ffnn_hid_size=288,dilation=[1, 2, 3],conv_dropout=0.5,emb_dropout=0.5,out_dropout=0.33,**kwargs):\n",
    "        config = NeZhaConfig.from_pretrained(\"nezha-cn-base\", num_labels=len({'<none>': 0, '<suc>': 1, 'LOC': 2}), output_hidden_states=True)\n",
    "        super(W2NERNezha, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.use_bert_last_4_layers = use_bert_last_4_layers\n",
    "\n",
    "        self.lstm_hid_size = lstm_hid_size\n",
    "        self.conv_hid_size = conv_hid_size\n",
    "\n",
    "        lstm_input_size = 0\n",
    "\n",
    "        self.bert = NeZhaModel.from_pretrained(\"nezha-cn-base\", config=config)\n",
    "        lstm_input_size += config.hidden_size\n",
    "\n",
    "        self.dis_embs = nn.Embedding(20, dist_emb_size)\n",
    "        self.reg_embs = nn.Embedding(3, type_emb_size)\n",
    "\n",
    "        self.encoder = nn.LSTM(lstm_input_size, lstm_hid_size // 2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        conv_input_size = lstm_hid_size + dist_emb_size + type_emb_size\n",
    "\n",
    "        self.convLayer = ConvolutionLayer(conv_input_size, conv_hid_size, dilation, conv_dropout)\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.predictor = CoPredictor(self.num_labels, lstm_hid_size, biaffine_size, conv_hid_size * len(dilation), ffnn_hid_size, out_dropout)\n",
    "\n",
    "        self.cln = LayerNorm(lstm_hid_size, lstm_hid_size, conditional=True)\n",
    "\n",
    "    def forward( self, input_ids, attention_mask, token_type_ids, grid_mask2d, dist_inputs, pieces2word, input_lengths, **kwargs):\n",
    "        bert_embs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        if self.use_bert_last_4_layers:\n",
    "            bert_embs = torch.stack(bert_embs[2][-4:], dim=-1).mean(-1)\n",
    "        else:\n",
    "            bert_embs = bert_embs[0]\n",
    "\n",
    "        length = pieces2word.size(1)\n",
    "\n",
    "        min_value = torch.min(bert_embs).item()\n",
    "\n",
    "        # Max pooling word representations from pieces\n",
    "        _bert_embs = bert_embs.unsqueeze(1).expand(-1, length, -1, -1)\n",
    "        _bert_embs = torch.masked_fill(_bert_embs, pieces2word.eq(0).unsqueeze(-1), min_value)\n",
    "        word_reps, _ = torch.max(_bert_embs, dim=2)\n",
    "\n",
    "        word_reps = self.dropout(word_reps)\n",
    "        packed_embs = pack_padded_sequence(word_reps, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_outs, (hidden, _) = self.encoder(packed_embs)\n",
    "        # 源码每个batch的长度等于 input_lengths.max()\n",
    "        word_reps, _ = pad_packed_sequence(packed_outs, batch_first=True, total_length=length)\n",
    "\n",
    "        cln = self.cln(word_reps.unsqueeze(2), word_reps)\n",
    "\n",
    "        dis_emb = self.dis_embs(dist_inputs)\n",
    "        tril_mask = torch.tril(grid_mask2d.clone().long())\n",
    "        reg_inputs = tril_mask + grid_mask2d.clone().long()\n",
    "        reg_emb = self.reg_embs(reg_inputs)\n",
    "\n",
    "        conv_inputs = torch.cat([dis_emb, reg_emb, cln], dim=-1)\n",
    "        conv_inputs = torch.masked_fill(conv_inputs, grid_mask2d.eq(0).unsqueeze(-1), 0.0)\n",
    "        conv_outputs = self.convLayer(conv_inputs)\n",
    "        conv_outputs = torch.masked_fill(conv_outputs, grid_mask2d.eq(0).unsqueeze(-1), 0.0)\n",
    "        outputs = self.predictor(word_reps, word_reps, conv_outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nezha():\n",
    "    print(\"/\" * 50, f\" finetuning nezha \", \"/\" * 50)\n",
    "    tokenizer = Tokenizer(vocab=\"nezha-cn-base\", max_seq_len=max_len)\n",
    "    train_data_df, dev_data_df = train_test_split(dataset, test_size=0.3)\n",
    "    ner_train_dataset = Dataset(train_data_df, categories=categories)\n",
    "    ner_dev_dataset = Dataset(dev_data_df, categories=categories)\n",
    "    ner_train_dataset.convert_to_ids(tokenizer)\n",
    "    ner_dev_dataset.convert_to_ids(tokenizer)\n",
    "\n",
    "    show_step = len(ner_train_dataset) // batch_size + 2\n",
    "    t_total = len(ner_train_dataset) // batch_size * num_epoches\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    dl_module = W2NERNezha.from_pretrained(\"nezha-cn-base\")\n",
    "\n",
    "    optimizer = get_default_w2ner_optimizer(dl_module, lr=1e-2, bert_lr=5e-5, weight_decay=0.01)\n",
    "    scheduler = get_default_cosine_schedule_with_warmup(optimizer, t_total, warmup_ratio=0.2)\n",
    "    \n",
    "    model = Task(dl_module, optimizer, 'ce', cude_device=device, scheduler=scheduler, grad_clip=5.0, ema_decay=0.995, fgm_attack=True, save_path=f\"outputs/nezha-cn-base-finetuned\", )\n",
    "    model.fit(ner_train_dataset, ner_dev_dataset, epochs=num_epoches, batch_size=batch_size, show_step=show_step)\n",
    "\n",
    "    # BertTokenizer.from_pretrained(\"nezha-cn-base\").save_pretrained(f\"outputs/nezha-cn-base-finetuned\")\n",
    "\n",
    "    gc.collect()\n",
    "    del train_data_df, dev_data_df, ner_train_dataset, ner_dev_dataset, dl_module\n",
    "    print(\"/\" * 110)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train_nezha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_roberta_cv():\n",
    "    for i, (train_index, dev_index) in enumerate(kf.split(dataset), start=1):\n",
    "        print(\"/\" * 50, f\" cv {i} \", \"/\" * 50)\n",
    "        tokenizer = Tokenizer(vocab=model_name, max_seq_len=max_len)\n",
    "\n",
    "        train_data_df, dev_data_df = dataset.loc[train_index], dataset.loc[dev_index]\n",
    "        ner_train_dataset = Dataset(train_data_df, categories=categories)\n",
    "        ner_dev_dataset = Dataset(dev_data_df, categories=categories)\n",
    "        ner_train_dataset.convert_to_ids(tokenizer)\n",
    "        ner_dev_dataset.convert_to_ids(tokenizer)\n",
    "\n",
    "        show_step = len(ner_train_dataset) // batch_size + 2\n",
    "        t_total = len(ner_train_dataset) // batch_size * num_epoches\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        config = W2NERBertConfig.from_pretrained(model_name, num_labels=len(ner_train_dataset.cat2id))\n",
    "        dl_module = W2NERBert.from_pretrained(model_name, config=config)\n",
    "\n",
    "        optimizer = get_default_w2ner_optimizer(dl_module, lr=1e-2, bert_lr=5e-5, weight_decay=0.01)\n",
    "        scheduler = get_default_cosine_schedule_with_warmup(optimizer, t_total, warmup_ratio=0.1)\n",
    "        \n",
    "        model = Task(dl_module, optimizer, 'ce', cude_device=device, scheduler=scheduler, grad_clip=5.0, ema_decay=0.995, fgm_attack=True, save_path=f\"outputs/roberta-kflod-{i}\", )\n",
    "        model.fit(ner_train_dataset, ner_dev_dataset, epochs=num_epoches, batch_size=batch_size, show_step=show_step)\n",
    "\n",
    "        # AutoTokenizer.from_pretrained(model_name).save_pretrained(f\"outputs/roberta-kflod-{i}\")\n",
    "        \n",
    "        gc.collect()\n",
    "        del train_data_df, dev_data_df, ner_train_dataset, ner_dev_dataset, model, dl_module\n",
    "        \n",
    "        print(\"/\" * 108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_roberta_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from ark_nlp.model.ner.w2ner_bert import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFW2NERPredictor(Predictor):\n",
    "    def E_trans_to_C(self, string):\n",
    "        E_pun = u',.!?[]()<>\"\\''\n",
    "        C_pun = u'，。！？【】（）《》“‘'\n",
    "        table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "\n",
    "        return string.translate(table)\n",
    "\n",
    "    def predict_one_sample(self, text='', prompt=None, cv=False):\n",
    "        text = text.strip()\n",
    "        \n",
    "        features = self._get_input_ids(E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", text)), prompt=prompt)\n",
    "        self.module.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = self._get_module_one_sample_inputs(features)\n",
    "            logit = self.module(**inputs)\n",
    "\n",
    "        preds = torch.argmax(logit, -1)\n",
    "\n",
    "        instance, l = preds.cpu().numpy()[0], int(inputs['input_lengths'].cpu().numpy()[0])\n",
    "\n",
    "        forward_dict = {}\n",
    "        head_dict = {}\n",
    "        ht_type_dict = {}\n",
    "        for i in range(l):\n",
    "            for j in range(i + 1, l):\n",
    "                if instance[i, j] == 1:\n",
    "                    if i not in forward_dict:\n",
    "                        forward_dict[i] = [j]\n",
    "                    else:\n",
    "                        forward_dict[i].append(j)\n",
    "        for i in range(l):\n",
    "            for j in range(i, l):\n",
    "                if instance[j, i] > 1:\n",
    "                    ht_type_dict[(i, j)] = instance[j, i]\n",
    "                    if i not in head_dict:\n",
    "                        head_dict[i] = {j}\n",
    "                    else:\n",
    "                        head_dict[i].add(j)\n",
    "\n",
    "        predicts = []\n",
    "\n",
    "        def find_entity(key, entity, tails):\n",
    "            entity.append(key)\n",
    "            if key not in forward_dict:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "                entity.pop()\n",
    "                return\n",
    "            else:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "            for k in forward_dict[key]:\n",
    "                find_entity(k, entity, tails)\n",
    "            entity.pop()\n",
    "\n",
    "        for head in head_dict:\n",
    "            find_entity(head, [], head_dict[head])\n",
    "\n",
    "        entities = []\n",
    "        for entity_ in predicts:\n",
    "            entities.append({\n",
    "                \"idx\": entity_,\n",
    "                \"entity\": ''.join([text[i] for i in entity_]),\n",
    "                \"type\": self.id2cat[ht_type_dict[(entity_[0], entity_[-1])]]\n",
    "            })\n",
    "\n",
    "        if cv:\n",
    "            return text, int(inputs['input_lengths'].cpu().numpy()[0]), logit.cpu().numpy()\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def get_result(self, text, text_len, logit):\n",
    "        preds = np.argmax(logit, -1)\n",
    "\n",
    "        instance, l = preds[0], text_len\n",
    "\n",
    "        forward_dict = {}\n",
    "        head_dict = {}\n",
    "        ht_type_dict = {}\n",
    "        for i in range(l):\n",
    "            for j in range(i + 1, l):\n",
    "                if instance[i, j] == 1:\n",
    "                    if i not in forward_dict:\n",
    "                        forward_dict[i] = [j]\n",
    "                    else:\n",
    "                        forward_dict[i].append(j)\n",
    "        for i in range(l):\n",
    "            for j in range(i, l):\n",
    "                if instance[j, i] > 1:\n",
    "                    ht_type_dict[(i, j)] = instance[j, i]\n",
    "                    if i not in head_dict:\n",
    "                        head_dict[i] = {j}\n",
    "                    else:\n",
    "                        head_dict[i].add(j)\n",
    "\n",
    "        predicts = []\n",
    "\n",
    "        def find_entity(key, entity, tails):\n",
    "            entity.append(key)\n",
    "            if key not in forward_dict:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "                entity.pop()\n",
    "                return\n",
    "            else:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "            for k in forward_dict[key]:\n",
    "                find_entity(k, entity, tails)\n",
    "            entity.pop()\n",
    "\n",
    "        for head in head_dict:\n",
    "            find_entity(head, [], head_dict[head])\n",
    "\n",
    "        entities = []\n",
    "        for entity_ in predicts:\n",
    "            entities.append({\n",
    "                \"idx\": entity_,\n",
    "                \"entity\": ''.join([text[i] for i in entity_]),\n",
    "                \"type\": self.id2cat[ht_type_dict[(entity_[0], entity_[-1])]]\n",
    "            })\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\", sep=\"\\t\")\n",
    "test[\"text\"] = test[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", line.strip())))\n",
    "test_dateset = test[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_cv():\n",
    "#     predict_results_cv = [[] for i in range(len(test))]\n",
    "\n",
    "#     for i in range(1, cv + 1):\n",
    "#         model_name = f\"./outputs/roberta-kflod-{i}.pkl\"\n",
    "        \n",
    "#         tokenizer = Tokenizer(vocab=model_name, max_seq_len=max_len)\n",
    "#         config = W2NERBertConfig.from_pretrained(model_name, num_labels=len({'<none>': 0, '<suc>': 1, 'LOC': 2}))\n",
    "#         module = W2NERBert.from_pretrained(model_name, config=config).to(torch.device(f\"cuda:{device}\"))\n",
    "\n",
    "#         ner_predictor_instance = Predictor(module, tokenizer, {'<none>': 0, '<suc>': 1, 'LOC': 2})\n",
    "        \n",
    "#         for index, _line in tqdm(enumerate(test_dateset), desc=f\"{model_name} inference: \", total=len(test_dateset)):\n",
    "#             predict_results_cv[index].extend([_preditc[\"entity\"] for _preditc in ner_predictor_instance.predict_one_sample(_line)])\n",
    "\n",
    "#     predict_results = []\n",
    "#     for predict_ in predict_results_cv:\n",
    "#         predict_results.append([k for k, v in dict(Counter(predict_)).items() if v > 3] if predict_ else [])\n",
    "    \n",
    "#     return predict_results, predict_results_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2657/2657 [06:44<00:00,  6.57it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_results = {\"avg\": [], \"sum\": []}\n",
    "pseudo_data = []\n",
    "\n",
    "models_path = ['roberta-kflod-1_best.pkl', 'roberta-kflod-2.pkl', 'roberta-kflod-2_best.pkl', 'roberta-kflod-3_best.pkl', 'roberta-kflod-3.pkl', 'roberta-kflod-5.pkl', 'roberta-kflod-4_best.pkl', 'roberta-kflod-1.pkl', 'roberta-kflod-5_best.pkl', 'roberta-kflod-4.pkl']\n",
    "tokenizer = Tokenizer(vocab=model_name, max_seq_len=max_len)\n",
    "ner_predictor_instances = [IFW2NERPredictor(torch.load(f\"./outputs/{f}\"), tokenizer, {'<none>': 0, '<suc>': 1, 'LOC': 2}) for f in models_path]\n",
    "\n",
    "for _line in tqdm(test_dateset):\n",
    "    logits = []\n",
    "    for ner_predictor_instance in ner_predictor_instances:\n",
    "        text, text_len, logit = ner_predictor_instance.predict_one_sample(_line, cv=True)\n",
    "        logits.append(logit)\n",
    "\n",
    "    logit_sum = np.sum(np.array(logits), axis=0)\n",
    "    logit_avg = np.mean(np.array(logits), axis=0)\n",
    "    \n",
    "    label_sum = set()\n",
    "    label_avg = set()\n",
    "    for _preditc in ner_predictor_instances[0].get_result(text, text_len, logit_sum):\n",
    "        label_sum.add(_preditc[\"entity\"])\n",
    "    \n",
    "    for _preditc in ner_predictor_instances[0].get_result(text, text_len, logit_avg):\n",
    "        label_avg.add(_preditc[\"entity\"])\n",
    "    \n",
    "    label_sum = list(label_sum)\n",
    "    label_avg = list(label_avg)\n",
    "\n",
    "    predict_results[\"sum\"].append(label_sum)\n",
    "    predict_results[\"avg\"].append(label_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nezha(model=None):\n",
    "    predict_results = [[] for i in range(len(test))]\n",
    "    model_name = f\"./outputs/nezha-cn-base-finetuned.pkl\"\n",
    "\n",
    "    tokenizer = Tokenizer(vocab=\"nezha-cn-base\", max_seq_len=max_len)\n",
    "    \n",
    "    if model is None:\n",
    "        # module = W2NERNezha.from_pretrained(model_name)\n",
    "        module = torch.load(model_name)\n",
    "    else:\n",
    "        module = model.module\n",
    "        \n",
    "    ner_predictor_instance = IFW2NERPredictor(module, tokenizer, {'<none>': 0, '<suc>': 1, 'LOC': 2})\n",
    "    \n",
    "    for index, _line in tqdm(enumerate(test_dateset), desc=f\"{model_name} inference: \", total=len(test_dateset)):\n",
    "        predict_results[index].extend([_preditc[\"entity\"] for _preditc in ner_predictor_instance.predict_one_sample(_line)])\n",
    "    \n",
    "    return [list(set(i)) for i in predict_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results, predict_results_cv = predict_cv()\n",
    "# predict_results = predict_nezha(model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2ner_nezha_submit.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"tag\\n\")\n",
    "    for _result in predict_results:\n",
    "       f.write(f\"{str(_result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo_data = pd.DataFrame(pseudo_data, columns=[\"text\", \"tag\"])\n",
    "# pseudo_data.to_csv(\"data/pseudo.csv\", index=False, encoding=\"utf-8\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method, predict_result in predict_results.items():\n",
    "    with open(f'w2ner_submit_cv_{method}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"tag\\n\")\n",
    "        for _result in predict_result:\n",
    "            f.write(f\"{str(_result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91d8ac8ac98a00daee01b170ecc4de38a4b78e57473b1984dedfa9b67acb5aae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
