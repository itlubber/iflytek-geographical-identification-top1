{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from ark_nlp.model.ner.w2ner_bert import W2NERBert\n",
    "from ark_nlp.model.ner.w2ner_bert import W2NERBertConfig\n",
    "from ark_nlp.model.ner.w2ner_bert import Dataset\n",
    "from ark_nlp.model.ner.w2ner_bert import Task\n",
    "from ark_nlp.model.ner.w2ner_bert import get_default_w2ner_optimizer\n",
    "from ark_nlp.factory.lr_scheduler import get_default_linear_schedule_with_warmup, get_default_cosine_schedule_with_warmup\n",
    "from ark_nlp.model.ner.w2ner_bert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "tqdm.pandas(desc=\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_trans_to_C(string):\n",
    "    E_pun = u',.!?[]()<>\"\\''\n",
    "    C_pun = u'，。！？【】（）《》“‘'\n",
    "    table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "    return string.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\", sep=\"\\t\")\n",
    "train = pd.read_csv(\"data/train.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"text\"] = test[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"tag\"] = train[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"entities\"] = train.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'entities': entity_labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(datalist)\n",
    "train_data_df, dev_data_df = train_test_split(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta = pd.read_csv(\"data/tta.csv\", sep=\"\\t\")\n",
    "tta[\"text\"] = tta[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "tta[\"tag\"] = tta[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])\n",
    "tta[\"entities\"] = tta.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)\n",
    "\n",
    "tta_datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    tta_datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'entities': entity_labels\n",
    "    })\n",
    "\n",
    "tta_data = pd.DataFrame(tta_datalist)\n",
    "train_data_df = pd.concat([train_data_df, tta_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(x):\n",
    "    \n",
    "    entities = []\n",
    "    for entity in x:\n",
    "        entity_ = {}\n",
    "        idx = list(range(entity['start_idx'], entity['end_idx']))\n",
    "        entity_['idx'] = idx\n",
    "        entity_['type'] = entity['type']\n",
    "        entity_['entity'] = entity['entity']\n",
    "        entities.append(entity_)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['label'] = train_data_df['entities'].apply(lambda x: get_label(x))\n",
    "dev_data_df['label'] = dev_data_df['entities'].apply(lambda x: get_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df.loc[:,['text', 'label']]\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "dev_data_df = dev_data_df.loc[:,['text', 'label']]\n",
    "dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset = Dataset(train_data_df)\n",
    "ner_dev_dataset = Dataset(dev_data_df, categories=ner_train_dataset.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab='./outputs/roberta-finetuned-cosine', max_seq_len=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset.convert_to_ids(tokenizer)\n",
    "ner_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = W2NERBertConfig.from_pretrained('./outputs/roberta-finetuned-cosine', num_labels=len(ner_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_module = W2NERBert.from_pretrained('./outputs/roberta-finetuned-cosine', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches, batch_size = 25, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_default_w2ner_optimizer(dl_module, lr=5e-3, bert_lr=2.5e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意lr衰减轮次的设定\n",
    "show_step = len(ner_train_dataset) // batch_size + 2\n",
    "t_total = len(ner_train_dataset) // batch_size * num_epoches\n",
    "scheduler = get_default_cosine_schedule_with_warmup(optimizer, t_total, warmup_ratio=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, 'ce', cude_device=2, scheduler=scheduler, grad_clip=10.0, ema_decay=0.995, fgm_attack=True, save_path=\"outputs/roberta-finetuned-tta\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ner_train_dataset, ner_dev_dataset, epochs=num_epoches, batch_size=batch_size, show_step=show_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.model.ner.w2ner_bert import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_predictor_instance = Predictor(model.module, tokenizer, ner_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = []\n",
    "tta_data = []\n",
    "\n",
    "for _line in tqdm(test[\"text\"].tolist()):\n",
    "    label = set()\n",
    "    for _preditc in ner_predictor_instance.predict_one_sample(_line):\n",
    "        label.add(_preditc[\"entity\"])\n",
    "    \n",
    "    label = list(label)\n",
    "    if len(label) > 0:\n",
    "        tta_data.append([_line, label])\n",
    "\n",
    "    predict_results.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2ner_submit_tta.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"tag\\n\")\n",
    "    for _result in predict_results:\n",
    "       f.write(f\"{str(_result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_data = pd.DataFrame(tta_data, columns=[\"text\", \"tag\"])\n",
    "tta_data.to_csv(\"data/tta_finetune.csv\", index=False, encoding=\"utf-8\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoTokenizer.from_pretrained('./outputs/roberta-finetuned-cosine').save_pretrained(\"outputs/roberta-finetuned-tta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91d8ac8ac98a00daee01b170ecc4de38a4b78e57473b1984dedfa9b67acb5aae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
