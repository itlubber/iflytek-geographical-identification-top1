{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from ark_nlp.model.ner.w2ner_bert import W2NERBertConfig\n",
    "from ark_nlp.model.ner.w2ner_bert import Tokenizer\n",
    "from ark_nlp.model.ner.w2ner_bert import W2NERBert\n",
    "from ark_nlp.model.ner.w2ner_bert import Dataset\n",
    "from ark_nlp.model.ner.w2ner_bert import Task\n",
    "from ark_nlp.model.ner.w2ner_bert import get_default_w2ner_optimizer\n",
    "from ark_nlp.factory.lr_scheduler import get_default_linear_schedule_with_warmup, get_default_cosine_schedule_with_warmup\n",
    "from ark_nlp.factory.utils.seed import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "tqdm.pandas(desc=\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_trans_to_C(string):\n",
    "    E_pun = u',.!?[]()<>\"\\''\n",
    "    C_pun = u'，。！？【】（）《》“‘'\n",
    "    table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "    return string.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\", sep=\"\\t\")\n",
    "train = pd.read_csv(\"data/train.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"text\"] = test[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "train[\"tag\"] = train[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.finditer(\"[\\(《：；→，。、\\-”]+$\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"entities\"] = train.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'entities': entity_labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.DataFrame(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo = pd.read_csv(\"data/pseudo_best.csv\", sep=\"\\t\")\n",
    "pseudo[\"text\"] = pseudo[\"text\"].apply(lambda line: E_trans_to_C(re.sub(\"[\\(《：→；，。、\\-”]+$\", \"\", line.strip())))\n",
    "pseudo[\"tag\"] = pseudo[\"tag\"].apply(lambda x: [E_trans_to_C(i) for i in eval(str(x))])\n",
    "pseudo[\"entities\"] = pseudo.progress_apply(lambda row: [[\"LOC\", *i.span()] for tag in row[\"tag\"] for i in re.finditer(tag, row[\"text\"])], axis=1)\n",
    "\n",
    "pseudo_datalist = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    entity_labels = []\n",
    "    for _type, _start_idx, _end_idx in row[\"entities\"]:\n",
    "        entity_labels.append({\n",
    "            'start_idx': _start_idx,\n",
    "            'end_idx': _end_idx,\n",
    "            'type': _type,\n",
    "            'entity': row[\"text\"][_start_idx: _end_idx]\n",
    "    })\n",
    "\n",
    "    pseudo_datalist.append({\n",
    "        'text': row[\"text\"],\n",
    "        'entities': entity_labels\n",
    "    })\n",
    "\n",
    "pseudo_data = pd.DataFrame(pseudo_datalist)\n",
    "train_data_df = pd.concat([train_data_df, pseudo_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(x):\n",
    "    \n",
    "    entities = []\n",
    "    for entity in x:\n",
    "        entity_ = {}\n",
    "        idx = list(range(entity['start_idx'], entity['end_idx']))\n",
    "        entity_['idx'] = idx\n",
    "        entity_['type'] = entity['type']\n",
    "        entity_['entity'] = entity['entity']\n",
    "        entities.append(entity_)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['label'] = train_data_df['entities'].apply(lambda x: get_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df.loc[:,['text', 'label']]\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset = Dataset(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab='roberta-base-finetuned-cluener2020-chinese', max_seq_len=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = W2NERBertConfig.from_pretrained('roberta-base-finetuned-cluener2020-chinese', num_labels=len(ner_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_module = W2NERBert.from_pretrained('roberta-base-finetuned-cluener2020-chinese', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "# num_epoches, batch_size = 10, 16 # 0.91\n",
    "# num_epoches, batch_size = 15, 16\n",
    "num_epoches, batch_size = 40, 256 # 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = get_default_w2ner_optimizer(dl_module) # 0.91\n",
    "# optimizer = get_default_w2ner_optimizer(dl_module, lr=5e-4, bert_lr=1e-5, weight_decay=0.01)\n",
    "# optimizer = get_default_w2ner_optimizer(dl_module, lr=5e-2, bert_lr=5e-5, weight_decay=0.01)\n",
    "optimizer = get_default_w2ner_optimizer(dl_module, lr=1e-2, bert_lr=5e-5, weight_decay=0.01) # 0.91143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意lr衰减轮次的设定\n",
    "show_step = len(ner_train_dataset) // batch_size + 2\n",
    "t_total = len(ner_train_dataset) // batch_size * num_epoches\n",
    "scheduler = get_default_cosine_schedule_with_warmup(optimizer, t_total, warmup_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, 'ce', cude_device=2, scheduler=scheduler, grad_clip=5.0, ema_decay=0.995, fgm_attack=True, save_path=\"outputs/roberta-finetuned-allpseudo\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(ner_train_dataset, epochs=num_epoches, batch_size=batch_size, show_step=show_step, lr=1e-5)\n",
    "model.fit(ner_train_dataset, epochs=num_epoches, batch_size=batch_size, show_step=show_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.model.ner.w2ner_bert import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFW2NERPredictor(Predictor):\n",
    "    def E_trans_to_C(self, string):\n",
    "        E_pun = u',.!?[]()<>\"\\''\n",
    "        C_pun = u'，。！？【】（）《》“‘'\n",
    "        table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "\n",
    "        return string.translate(table)\n",
    "\n",
    "    def predict_one_sample(self, text=''):\n",
    "        text = text.strip()\n",
    "        \n",
    "        features = self._get_input_ids(E_trans_to_C(re.sub(\"[\\(《：；→，。、\\-”]+$\", \"\", text)))\n",
    "        self.module.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = self._get_module_one_sample_inputs(features)\n",
    "            logit = self.module(**inputs)\n",
    "\n",
    "        preds = torch.argmax(logit, -1)\n",
    "\n",
    "        instance, l = preds.cpu().numpy()[0], int(inputs['input_lengths'].cpu().numpy()[0])\n",
    "\n",
    "        forward_dict = {}\n",
    "        head_dict = {}\n",
    "        ht_type_dict = {}\n",
    "        for i in range(l):\n",
    "            for j in range(i + 1, l):\n",
    "                if instance[i, j] == 1:\n",
    "                    if i not in forward_dict:\n",
    "                        forward_dict[i] = [j]\n",
    "                    else:\n",
    "                        forward_dict[i].append(j)\n",
    "        for i in range(l):\n",
    "            for j in range(i, l):\n",
    "                if instance[j, i] > 1:\n",
    "                    ht_type_dict[(i, j)] = instance[j, i]\n",
    "                    if i not in head_dict:\n",
    "                        head_dict[i] = {j}\n",
    "                    else:\n",
    "                        head_dict[i].add(j)\n",
    "\n",
    "        predicts = []\n",
    "\n",
    "        def find_entity(key, entity, tails):\n",
    "            entity.append(key)\n",
    "            if key not in forward_dict:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "                entity.pop()\n",
    "                return\n",
    "            else:\n",
    "                if key in tails:\n",
    "                    predicts.append(entity.copy())\n",
    "            for k in forward_dict[key]:\n",
    "                find_entity(k, entity, tails)\n",
    "            entity.pop()\n",
    "\n",
    "        for head in head_dict:\n",
    "            find_entity(head, [], head_dict[head])\n",
    "\n",
    "        entities = []\n",
    "        for entity_ in predicts:\n",
    "            entities.append({\n",
    "                \"idx\": entity_,\n",
    "                \"entity\": ''.join([text[i] for i in entity_]),\n",
    "                \"type\": self.id2cat[ht_type_dict[(entity_[0], entity_[-1])]]\n",
    "            })\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_predictor_instance = IFW2NERPredictor(model.module, tokenizer, ner_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = []\n",
    "pseudo_data = []\n",
    "\n",
    "for _line in tqdm(test[\"text\"].tolist()):\n",
    "    label = set()\n",
    "    for _preditc in ner_predictor_instance.predict_one_sample(_line):\n",
    "        label.add(_preditc[\"entity\"])\n",
    "    \n",
    "    label = list(label)\n",
    "    if len(label) > 0:\n",
    "        pseudo_data.append([_line, label])\n",
    "\n",
    "    predict_results.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2ner_submit_all_tta.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"tag\\n\")\n",
    "    for _result in predict_results:\n",
    "       f.write(f\"{str(_result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_data = pd.DataFrame(pseudo_data, columns=[\"text\", \"tag\"])\n",
    "pseudo_data.to_csv(\"data/pseudo_all_pseudo.csv\", index=False, encoding=\"utf-8\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer.from_pretrained('roberta-base-finetuned-cluener2020-chinese').save_pretrained(\"outputs/roberta-finetuned-allpseudo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
